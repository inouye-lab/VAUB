{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/local/scratch/a/gong123/AUB-CAUB/notebooks', '/local/scratch/a/gong123/anaconda3/envs/AUB-env/lib/python38.zip', '/local/scratch/a/gong123/anaconda3/envs/AUB-env/lib/python3.8', '/local/scratch/a/gong123/anaconda3/envs/AUB-env/lib/python3.8/lib-dynload', '', '/local/scratch/a/gong123/anaconda3/envs/AUB-env/lib/python3.8/site-packages', '/local/scratch/a/gong123/AUB-CAUB']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions.mixture_same_family import MixtureSameFamily\n",
    "import torch.distributions as DIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "\n",
    "added_path = os.path.join(os.path.abspath(\"..\"))\n",
    "if added_path not in sys.path:\n",
    "    sys.path.append(added_path)\n",
    "print(sys.path)\n",
    "\n",
    "from dataset.dataset_fair import AdultDataset, COMPAS"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T12:32:20.115212Z",
     "end_time": "2023-05-05T12:32:25.500660Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def conditional_errors(preds, labels, attrs):\n",
    "    \"\"\"\n",
    "    Compute the conditional errors of A = 0/1. All the arguments need to be one-dimensional vectors.\n",
    "    :param preds: The predicted label given by a model.\n",
    "    :param labels: The groundtruth label.\n",
    "    :param attrs: The label of sensitive attribute.\n",
    "    :return: Overall classification error, error | A = 0, error | A = 1.\n",
    "    \"\"\"\n",
    "    assert preds.shape == labels.shape and labels.shape == attrs.shape\n",
    "    cls_error = 1 - np.mean(preds == labels)\n",
    "    idx = attrs == 0\n",
    "    error_0 = 1 - np.mean(preds[idx] == labels[idx])\n",
    "    error_1 = 1 - np.mean(preds[~idx] == labels[~idx])\n",
    "    return cls_error, error_0, error_1\n",
    "\n",
    "def get_dataset_compas(root):\n",
    "    compas = pd.read_csv(f\"{root}/propublica.csv\").values\n",
    "    num_insts = compas.shape[0]\n",
    "    # Random shuffle the dataset.\n",
    "    indices = np.arange(num_insts)\n",
    "    np.random.shuffle(indices)\n",
    "    compas = compas[indices]\n",
    "    # Partition the dataset into train and test split.\n",
    "    ratio = 0.7\n",
    "    num_train = int(num_insts * ratio)\n",
    "    compas_train = COMPAS(compas[:num_train, :])\n",
    "    compas_test = COMPAS(compas[num_train:, :])\n",
    "    return compas_train, compas_test\n",
    "\n",
    "def get_loaders_compas(compas_train, compas_test, batch_size):\n",
    "    train_loader = DataLoader(compas_train, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(compas_test, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader ,test_loader\n",
    "\n",
    "\n",
    "def get_dataset_adult(root, target=\"income\", private=\"sex\"):\n",
    "    adult_train = AdultDataset(root_dir=root, phase='train', tar_attr=target, priv_attr=private)\n",
    "    adult_test = AdultDataset(root_dir=root, phase='test', tar_attr=target, priv_attr=private)\n",
    "    return adult_train, adult_test\n",
    "\n",
    "def get_loaders_adult(adult_train, adult_test, batch_size):\n",
    "    train_loader = DataLoader(adult_train, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(adult_test, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader ,test_loader\n",
    "\n",
    "def all_pairs_gaussian_kl(mu, sigma, dim_z):\n",
    "    # mu is [batchsize x dim_z]\n",
    "    # sigma is [batchsize x dim_z]\n",
    "\n",
    "    sigma_sq = sigma * sigma + 1e-8\n",
    "    sigma_sq_inv = 1 / sigma_sq\n",
    "    # sigma_inv is [batchsize x sizeof(latent_space)]\n",
    "\n",
    "    # first term\n",
    "    # dot product of all sigma_inv vectors with sigma\n",
    "    # is the same as a matrix mult of diag\n",
    "    first_term = torch.matmul(sigma_sq, sigma_sq_inv.transpose(1, 0))\n",
    "\n",
    "    # second term (we break the mu_i-mu_j square term)\n",
    "    # REMEMBER THAT THIS IS SIGMA_1, not SIGMA_0\n",
    "    sqi = torch.matmul(mu * mu, sigma_sq_inv.transpose(1, 0))\n",
    "    # sqi is now [batchsize x batchsize] = sum(mu[:,i]**2 / Sigma[j])\n",
    "\n",
    "    sqj = mu * mu * sigma_sq_inv\n",
    "    sqj = torch.sum(sqj, dim=1)\n",
    "    # sqj is now [batchsize, 1] = mu[j]**2 / Sigma[j]\n",
    "\n",
    "    # squared distance\n",
    "    # (mu[i] - mu[j])\\sigma_inv(mu[i] - mu[j]) = r[i] - 2*mu[i]*mu[j] + r[j]\n",
    "    # uses broadcasting\n",
    "    second_term = 2 * torch.matmul(mu, torch.transpose(mu * sigma_sq_inv, 1, 0))\n",
    "    second_term = sqi + sqj.view(1, -1) - second_term\n",
    "\n",
    "    # third term\n",
    "\n",
    "    # log det A = tr log A\n",
    "    # log \\frac{ det \\Sigma_1 }{ det \\Sigma_0 } =\n",
    "    #   \\tr\\log \\Sigma_1 - \\tr\\log \\Sigma_0\n",
    "    # for each sample, we have B comparisons to B other samples...\n",
    "    #   so this cancels out, but we keep it\n",
    "\n",
    "    logi = 2 * torch.sum(torch.log(sigma), dim=1)\n",
    "    logi = torch.reshape(logi, [-1, 1])\n",
    "    logj = torch.reshape(logi, [1, -1])\n",
    "    third_term = logi - logj\n",
    "\n",
    "    # combine and return\n",
    "    return 0.5 * (first_term + second_term + third_term - dim_z)\n",
    "\n",
    "class LinearVAUB(nn.Module):\n",
    "    def __init__(self, pz, cls, input_features=1, latent_features=1, hidden_features=3, learnable_loc=False, learnable_var=False, pair_kl=False):\n",
    "        super(LinearVAUB, self).__init__()\n",
    "\n",
    "        # encoder & decoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=hidden_features),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(in_features=hidden_features, out_features=hidden_features),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_features, out_features=latent_features*2),\n",
    "        )\n",
    "\n",
    "        # self.enc2 = nn.Sequential(\n",
    "        #     nn.Linear(in_features=input_features, out_features=hidden_features),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(in_features=hidden_features, out_features=hidden_features),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(in_features=hidden_features, out_features=latent_features*2),\n",
    "        # )\n",
    "\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_features, out_features=hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_features, out_features=hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_features, out_features=input_features),\n",
    "        )\n",
    "\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_features, out_features=hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_features, out_features=hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_features, out_features=input_features),\n",
    "        )\n",
    "\n",
    "        self.E_arr = [self.enc1, self.enc1]\n",
    "        self.D_arr = [self.dec1, self.dec2]\n",
    "\n",
    "        # shared distribution\n",
    "        '''\n",
    "        change this line\n",
    "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "        to\n",
    "        '''\n",
    "        self.log_scale_arr = [nn.Parameter(torch.Tensor([0.0])) for _ in range(2)]\n",
    "\n",
    "        self.latent_loc = nn.Parameter(torch.Tensor([0.0])) if learnable_loc else torch.Tensor([0.0])\n",
    "        self.latent_log_var = nn.Parameter(torch.Tensor([0.0])) if learnable_var else torch.Tensor([0.0])\n",
    "\n",
    "        self.Recon_Loss = [[],[]]\n",
    "\n",
    "        self.mu = None\n",
    "        self.log_var = None\n",
    "        self.elbo = None\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        self.pz = pz\n",
    "        self.classifier = cls\n",
    "        self.pair_kl = pair_kl\n",
    "\n",
    "    # def simulate_output(self, mean, idx):\n",
    "    #\n",
    "    #     noise = torch.distributions.Normal(0, 1)\n",
    "    #     return mean + noise.sample(mean.size())*0.05\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "\n",
    "        std = torch.exp(log_var/2).float()\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_encoded = self.E_arr[0](x)\n",
    "        return self.classifier(x_encoded)\n",
    "\n",
    "    def train_iter(self, X_arr, y_arr, scale_recon=1, lambda_cls=1):\n",
    "\n",
    "        self.recon_loss = 0\n",
    "        self.vaub_loss = 0\n",
    "        self.elbo_loss = 0\n",
    "        self.cls_loss = 0\n",
    "        self.kl_loss = 0\n",
    "\n",
    "        self.z_arr = []\n",
    "        self.x_hat_arr = []\n",
    "        self.x_flipped_arr = []\n",
    "\n",
    "        criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "        for idx in range(2):\n",
    "            x_original = X_arr[idx]\n",
    "            x = X_arr[idx]\n",
    "            # encoding\n",
    "            x = self.E_arr[idx](x)\n",
    "\n",
    "            # get `mu` and `log_var`\n",
    "            self.mu, self.log_var = x.chunk(2, dim=-1)\n",
    "            # self.mu = x[:, 0, None] # the first feature values as mean\n",
    "            # self.log_var = x[:, 1, None] # the other feature values as variance\n",
    "\n",
    "            # get the latent vector through reparameterization\n",
    "            z = self.reparameterize(self.mu, self.log_var)\n",
    "            self.z_arr.append(z)\n",
    "\n",
    "            # decoding\n",
    "            x_hat = self.D_arr[idx](z)\n",
    "            # self.x_hat_arr.append(self.simulate_output(x_hat, idx))\n",
    "\n",
    "            # get flipped results\n",
    "            x_flipped = self.D_arr[(idx+1)%2](z)\n",
    "            # self.x_flipped_arr.append(self.simulate_output(x_flipped,(idx+1)%2))\n",
    "\n",
    "            # reconstruction loss\n",
    "            recon_loss = -1*self.gaussian_likelihood(x_hat, x_original, self.log_scale_arr[idx])\n",
    "            self.Recon_Loss[idx].append(recon_loss.mean().item())\n",
    "\n",
    "            # kl loss\n",
    "            if self.pair_kl:\n",
    "                kl_loss = all_pairs_gaussian_kl(self.mu, torch.exp(self.log_var/2), self.mu.size(1))\n",
    "            else:\n",
    "                kl_loss = self.kl_divergence_with_pz(z, self.mu, self.log_var)\n",
    "\n",
    "            # elbo\n",
    "            elbo = kl_loss + scale_recon*recon_loss\n",
    "            vaub = kl_loss + recon_loss\n",
    "\n",
    "            self.kl_loss += kl_loss.mean()\n",
    "            self.recon_loss += recon_loss.mean()\n",
    "            self.elbo_loss += elbo.mean()\n",
    "            self.vaub_loss += vaub.mean()\n",
    "            self.cls_loss += criterion_cls(self.classifier(z), y_arr[idx])\n",
    "\n",
    "        return self.elbo_loss + lambda_cls*self.cls_loss\n",
    "\n",
    "    def get_loss_dict(self):\n",
    "        loss_dict = {\n",
    "            \"recon_loss\": self.recon_loss.item(),\n",
    "            \"kl_loss\": self.kl_loss.item(),\n",
    "            \"vaub_loss\": self.vaub_loss.item(),\n",
    "            \"elbo_loss\": self.elbo_loss.item(),\n",
    "            \"cls_loss\": self.cls_loss.item(),\n",
    "            \"overall_loss\": (self.elbo_loss + lambda_cls*self.cls_loss).item()\n",
    "        }\n",
    "        return loss_dict\n",
    "\n",
    "    def kl_divergence_with_pz(self, z, mu, log_var):\n",
    "\n",
    "        # mu = self.mu\n",
    "        std = torch.exp(log_var/2)\n",
    "\n",
    "        # p = torch.distributions.Normal(torch.zeros_like(mu)+self.latent_loc, torch.ones_like(std)*torch.exp(self.latent_log_var/2))\n",
    "        # p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std.to(mu.device))\n",
    "\n",
    "        log_qzx = q.log_prob(z)\n",
    "        # log_pz = p.log_prob(z)\n",
    "        log_pz = self.pz.log_prob(z)\n",
    "\n",
    "        kl = (log_qzx.sum(-1) - log_pz)\n",
    "\n",
    "        # sum over last dim to go from single dim distribution to multi-dim\n",
    "        return kl\n",
    "\n",
    "\n",
    "    # def kl_divergence(self, z, mu, log_var):\n",
    "    #\n",
    "    #     # mu = self.mu\n",
    "    #     std = torch.exp(log_var/2)\n",
    "    #\n",
    "    #     p = torch.distributions.Normal(torch.zeros_like(mu)+self.latent_loc, torch.ones_like(std)*torch.exp(self.latent_log_var/2))\n",
    "    #     # p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "    #     q = torch.distributions.Normal(mu, std)\n",
    "    #\n",
    "    #     log_qzx = q.log_prob(z).sum(-1)\n",
    "    #     log_pz = p.log_prob(z)\n",
    "    #\n",
    "    #     kl = (log_qzx - log_pz)\n",
    "    #\n",
    "    #     # sum over last dim to go from single dim distribution to multi-dim\n",
    "    #     kl = kl\n",
    "    #     return kl\n",
    "\n",
    "\n",
    "    def gaussian_likelihood(self, x_hat, x, learnable_scale):\n",
    "        scale = torch.exp(learnable_scale)\n",
    "        mean = x_hat\n",
    "        # print(mean.size(), scale.size())\n",
    "        dist = torch.distributions.Normal(mean, scale.to(device))\n",
    "\n",
    "        # measure prob of seeing image under p(x|z)\n",
    "        # print(mean.size(), x.size())\n",
    "        log_pxz = dist.log_prob(x)\n",
    "        # print(log_pxz.size(), log_pxz.sum(-1).size())\n",
    "        return log_pxz.sum(-1)\n",
    "\n",
    "\n",
    "def train_VAUB_fair(model, train_data_loader, test_dataset, n_epochs, lr, scale_recon, lambda_cls, device):\n",
    "\n",
    "    opt = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    Loss_dict = {\n",
    "        \"recon_loss\": [],\n",
    "        \"kl_loss\": [],\n",
    "        \"vaub_loss\": [],\n",
    "        \"elbo_loss\": [],\n",
    "        \"cls_loss\": [],\n",
    "        \"overall_loss\": []\n",
    "    }\n",
    "    target_insts = torch.from_numpy(test_dataset.X).float().to(device)\n",
    "    target_labels = np.argmax(test_dataset.Y, axis=1)\n",
    "    target_attrs = np.argmax(test_dataset.A, axis=1)\n",
    "    target_insts = torch.from_numpy(test_dataset.X).float().to(device)\n",
    "    test_idx = target_attrs == 0\n",
    "    conditional_idx = target_labels == 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_data_loader):\n",
    "            opt.zero_grad()\n",
    "            xs, ys, attrs = batch\n",
    "            X_arr = [xs[attrs==0].to(device), xs[attrs==1].to(device)]\n",
    "            y_arr = [ys[attrs==0].to(device), ys[attrs==1].to(device)]\n",
    "            loss = model.train_iter(X_arr, y_arr, scale_recon, lambda_cls)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            for k,v in model.get_loss_dict().items():\n",
    "                Loss_dict[k].append(v)\n",
    "\n",
    "        model.eval()\n",
    "        preds_labels = torch.max(model(target_insts), 1)[1].cpu().numpy()\n",
    "        cls_error, error_0, error_1 = conditional_errors(preds_labels, target_labels, target_attrs)\n",
    "        if epoch % 5 ==0:\n",
    "            print(f\"Epoch {epoch}/{n_epochs}: Loss {loss.item():.4f}  \" +\\\n",
    "                  \", \".join([f\"{k}: {Loss_dict[k][-1]:.2f}\" for k in Loss_dict])+\\\n",
    "                  f\"\\nOverall predicted error = {cls_error:.2f}, Err|A=0 = {error_0:.2f}, Err|A=1 = {error_1:.2f}\")\n",
    "\n",
    "\n",
    "    pred_0, pred_1 = np.mean(preds_labels[test_idx]), np.mean(preds_labels[~test_idx])\n",
    "    preds_labels = torch.max(model(target_insts), 1)[1].cpu().numpy()\n",
    "    cond_00 = np.mean(preds_labels[np.logical_and(test_idx, conditional_idx)])\n",
    "    cond_10 = np.mean(preds_labels[np.logical_and(~test_idx, conditional_idx)])\n",
    "    cond_01 = np.mean(preds_labels[np.logical_and(test_idx, ~conditional_idx)])\n",
    "    cond_11 = np.mean(preds_labels[np.logical_and(~test_idx, ~conditional_idx)])\n",
    "    cls_error, _, _ = conditional_errors(preds_labels, target_labels, target_attrs)\n",
    "    print(f\"Overall Error: {cls_error:.4f}\")\n",
    "    print(\"Joint Error: |Err|A=0 + Err|A=1| = {}\".format(error_0 + error_1))\n",
    "    print(\"Error Gap: |Err|A=0 - Err|A=1| = {}\".format(np.abs(error_0 - error_1)))\n",
    "    print(f\"DP Gap: |Pred=1|A=0 - Pred=1|A=1| = {np.abs(pred_0 - pred_1):.4e}\")\n",
    "    print(\"Equalized Odds Y = 0: |Pred = 1|A = 0, Y = 0 - Pred = 1|A = 1, Y = 0| = {}\".format(\n",
    "        np.abs(cond_00 - cond_10)))\n",
    "    print(\"Equalized Odds Y = 1: |Pred = 1|A = 0, Y = 1 - Pred = 1|A = 1, Y = 1| = {}\".format(\n",
    "    np.abs(cond_01 - cond_11)))\n",
    "\n",
    "    return Loss_dict\n",
    "\n",
    "class MoGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_components, input_dim, loc_init=None, scale_init=None, weight_init=None):\n",
    "\n",
    "        super(MoGNN, self).__init__()\n",
    "\n",
    "        if loc_init is None:\n",
    "            self.loc = nn.Parameter((torch.rand(n_components, input_dim)*2-1))\n",
    "        else:\n",
    "            self.loc = nn.Parameter(loc_init)\n",
    "\n",
    "        if scale_init is None:\n",
    "            self.log_scale = nn.Parameter(torch.zeros(n_components, input_dim))\n",
    "        else:\n",
    "            self.log_scale = nn.Parameter(torch.log(scale_init))\n",
    "\n",
    "        if weight_init is None:\n",
    "            self.raw_weight = nn.Parameter(torch.ones(n_components))\n",
    "        else:\n",
    "            self.raw_weight = nn.Parameter(torch.log(weight_init/(1-weight_init)))\n",
    "\n",
    "    def log_prob(self, Z):\n",
    "\n",
    "        self.loc = self.loc.to(Z.device)\n",
    "        self.log_scale = self.log_scale.to(Z.device)\n",
    "        self.raw_weight = self.raw_weight.to(Z.device)\n",
    "\n",
    "        # print(torch.sigmoid(self.raw_weight))\n",
    "        mix = torch.distributions.Categorical(torch.sigmoid(self.raw_weight))\n",
    "        comp = torch.distributions.Independent(torch.distributions.Normal(self.loc, torch.exp(self.log_scale)), 1)\n",
    "        gmm = torch.distributions.mixture_same_family.MixtureSameFamily(mix, comp)\n",
    "\n",
    "        return gmm.log_prob(Z)\n",
    "\n",
    "class FixedGaussian(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, scale=1.0, device='cpu'):\n",
    "        super(FixedGaussian, self).__init__()\n",
    "        self.dist = torch.distributions.MultivariateNormal(torch.zeros(input_dim).to(device), scale_tril=torch.diag(torch.ones(input_dim)*scale).to(device))\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        return self.dist.log_prob(x)\n",
    "\n",
    "    def sample(self, n_sample):\n",
    "        return self.dist.sample((n_sample,))\n",
    "\n",
    "def gmm(weight, loc, scale):\n",
    "    mix = DIST.Categorical(weight)\n",
    "    comp = DIST.Normal(loc, scale)\n",
    "    return MixtureSameFamily(mix, comp)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=32, hidden_dim=16):\n",
    "\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = self.fc_2(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T12:33:34.541243Z",
     "end_time": "2023-05-05T12:33:34.585570Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100: Loss 315.8213  recon_loss: 309.22, kl_loss: 6.60, vaub_loss: 315.82, elbo_loss: 315.82, cls_loss: 1.39, overall_loss: 315.82\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 5/100: Loss 309.1332  recon_loss: 305.75, kl_loss: 3.38, vaub_loss: 309.13, elbo_loss: 309.13, cls_loss: 1.40, overall_loss: 309.13\n",
      "Overall predicted error = 0.29, Err|A=0 = 0.34, Err|A=1 = 0.19\n",
      "Epoch 10/100: Loss 319.1642  recon_loss: 315.77, kl_loss: 3.39, vaub_loss: 319.16, elbo_loss: 319.16, cls_loss: 1.41, overall_loss: 319.16\n",
      "Overall predicted error = 0.39, Err|A=0 = 0.44, Err|A=1 = 0.31\n",
      "Epoch 15/100: Loss 322.8605  recon_loss: 318.10, kl_loss: 4.76, vaub_loss: 322.86, elbo_loss: 322.86, cls_loss: 1.42, overall_loss: 322.86\n",
      "Overall predicted error = 0.57, Err|A=0 = 0.59, Err|A=1 = 0.53\n",
      "Epoch 20/100: Loss 302.9802  recon_loss: 297.83, kl_loss: 5.15, vaub_loss: 302.98, elbo_loss: 302.98, cls_loss: 1.39, overall_loss: 302.98\n",
      "Overall predicted error = 0.59, Err|A=0 = 0.61, Err|A=1 = 0.55\n",
      "Epoch 25/100: Loss 301.7954  recon_loss: 294.91, kl_loss: 6.88, vaub_loss: 301.80, elbo_loss: 301.80, cls_loss: 1.44, overall_loss: 301.80\n",
      "Overall predicted error = 0.66, Err|A=0 = 0.67, Err|A=1 = 0.64\n",
      "Epoch 30/100: Loss 300.0609  recon_loss: 293.19, kl_loss: 6.88, vaub_loss: 300.06, elbo_loss: 300.06, cls_loss: 1.43, overall_loss: 300.06\n",
      "Overall predicted error = 0.66, Err|A=0 = 0.64, Err|A=1 = 0.71\n",
      "Epoch 35/100: Loss 310.6561  recon_loss: 303.65, kl_loss: 7.01, vaub_loss: 310.66, elbo_loss: 310.66, cls_loss: 1.43, overall_loss: 310.66\n",
      "Overall predicted error = 0.62, Err|A=0 = 0.58, Err|A=1 = 0.71\n",
      "Epoch 40/100: Loss 279.7380  recon_loss: 271.87, kl_loss: 7.86, vaub_loss: 279.74, elbo_loss: 279.74, cls_loss: 1.45, overall_loss: 279.74\n",
      "Overall predicted error = 0.64, Err|A=0 = 0.59, Err|A=1 = 0.75\n",
      "Epoch 45/100: Loss 295.5922  recon_loss: 287.39, kl_loss: 8.20, vaub_loss: 295.59, elbo_loss: 295.59, cls_loss: 1.41, overall_loss: 295.59\n",
      "Overall predicted error = 0.65, Err|A=0 = 0.60, Err|A=1 = 0.76\n",
      "Epoch 50/100: Loss 284.1898  recon_loss: 275.51, kl_loss: 8.68, vaub_loss: 284.19, elbo_loss: 284.19, cls_loss: 1.43, overall_loss: 284.19\n",
      "Overall predicted error = 0.64, Err|A=0 = 0.59, Err|A=1 = 0.75\n",
      "Epoch 55/100: Loss 282.6197  recon_loss: 274.06, kl_loss: 8.56, vaub_loss: 282.62, elbo_loss: 282.62, cls_loss: 1.43, overall_loss: 282.62\n",
      "Overall predicted error = 0.65, Err|A=0 = 0.61, Err|A=1 = 0.76\n",
      "Epoch 60/100: Loss 279.7362  recon_loss: 271.41, kl_loss: 8.32, vaub_loss: 279.74, elbo_loss: 279.74, cls_loss: 1.44, overall_loss: 279.74\n",
      "Overall predicted error = 0.68, Err|A=0 = 0.63, Err|A=1 = 0.79\n",
      "Epoch 65/100: Loss 273.1432  recon_loss: 265.22, kl_loss: 7.92, vaub_loss: 273.14, elbo_loss: 273.14, cls_loss: 1.45, overall_loss: 273.14\n",
      "Overall predicted error = 0.68, Err|A=0 = 0.63, Err|A=1 = 0.78\n",
      "Epoch 70/100: Loss 291.1492  recon_loss: 281.44, kl_loss: 9.70, vaub_loss: 291.15, elbo_loss: 291.15, cls_loss: 1.42, overall_loss: 291.15\n",
      "Overall predicted error = 0.68, Err|A=0 = 0.64, Err|A=1 = 0.78\n",
      "Epoch 75/100: Loss 269.8078  recon_loss: 260.70, kl_loss: 9.11, vaub_loss: 269.81, elbo_loss: 269.81, cls_loss: 1.44, overall_loss: 269.81\n",
      "Overall predicted error = 0.67, Err|A=0 = 0.63, Err|A=1 = 0.76\n",
      "Epoch 80/100: Loss 278.1909  recon_loss: 268.92, kl_loss: 9.27, vaub_loss: 278.19, elbo_loss: 278.19, cls_loss: 1.45, overall_loss: 278.19\n",
      "Overall predicted error = 0.65, Err|A=0 = 0.61, Err|A=1 = 0.73\n",
      "Epoch 85/100: Loss 278.0701  recon_loss: 267.70, kl_loss: 10.37, vaub_loss: 278.07, elbo_loss: 278.07, cls_loss: 1.45, overall_loss: 278.07\n",
      "Overall predicted error = 0.64, Err|A=0 = 0.61, Err|A=1 = 0.71\n",
      "Epoch 90/100: Loss 269.7516  recon_loss: 261.95, kl_loss: 7.80, vaub_loss: 269.75, elbo_loss: 269.75, cls_loss: 1.43, overall_loss: 269.75\n",
      "Overall predicted error = 0.66, Err|A=0 = 0.62, Err|A=1 = 0.74\n",
      "Epoch 95/100: Loss 269.6411  recon_loss: 260.65, kl_loss: 8.99, vaub_loss: 269.64, elbo_loss: 269.64, cls_loss: 1.44, overall_loss: 269.64\n",
      "Overall predicted error = 0.67, Err|A=0 = 0.63, Err|A=1 = 0.74\n",
      "Overall Error: 0.6535\n",
      "Joint Error: |Err|A=0 + Err|A=1| = 1.3543114466878912\n",
      "Error Gap: |Err|A=0 - Err|A=1| = 0.1360203261596562\n",
      "DP Gap: |Pred=1|A=0 - Pred=1|A=1| = 7.7176e-02\n",
      "Equalized Odds Y = 0: |Pred = 1|A = 0, Y = 0 - Pred = 1|A = 1, Y = 0| = 0.04188764756751884\n",
      "Equalized Odds Y = 1: |Pred = 1|A = 0, Y = 1 - Pred = 1|A = 1, Y = 1| = 0.24227673019922302\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "#####     Training hyperparameters     #####\n",
    "############################################\n",
    "data_dir = \"/local/scratch/a/gong123/AUB-CAUB/ICLR2020-CFair/data\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 512\n",
    "n_epochs = 100\n",
    "lr = 1e-3\n",
    "input_features = 114\n",
    "hidden_features = 84\n",
    "latent_features = 60\n",
    "scale_recon = 1\n",
    "lambda_cls = 1e-15\n",
    "\n",
    "kwargs_pz = {\n",
    "    \"n_components\": 10,\n",
    "    \"input_dim\": latent_features,\n",
    "    }\n",
    "\n",
    "############################################\n",
    "#####         Training models          #####\n",
    "############################################\n",
    "pz = MoGNN(**kwargs_pz)\n",
    "cls = Classifier(input_dim=latent_features)\n",
    "VAUB = LinearVAUB(\n",
    "    pz = pz,\n",
    "    cls = cls,\n",
    "    input_features=input_features,\n",
    "    hidden_features=hidden_features,\n",
    "    latent_features=latent_features,\n",
    ")\n",
    "\n",
    "############################################\n",
    "#####          Training data           #####\n",
    "############################################\n",
    "train_dataset, test_dataset = get_dataset_adult(data_dir)\n",
    "train_loader, test_loader = get_loaders_adult(train_dataset, test_dataset, batch_size)\n",
    "\n",
    "############################################\n",
    "#####        Training script           #####\n",
    "############################################\n",
    "# for name, p in VAUB.named_parameters():\n",
    "#     if p.requires_grad == True:\n",
    "#         print(name)\n",
    "Loss_dict = train_VAUB_fair(VAUB, train_loader, test_dataset, n_epochs, lr, scale_recon, lambda_cls, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T12:38:12.429754Z",
     "end_time": "2023-05-05T12:40:10.860683Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/200: Loss 302.8466  recon_loss: 301.62, kl_loss: 1.23, vaub_loss: 302.85, elbo_loss: 302.85, cls_loss: 1.30, overall_loss: 302.85\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 5/200: Loss 310.2605  recon_loss: 310.07, kl_loss: 0.19, vaub_loss: 310.26, elbo_loss: 310.26, cls_loss: 1.28, overall_loss: 310.26\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 10/200: Loss 341.1550  recon_loss: 341.04, kl_loss: 0.11, vaub_loss: 341.15, elbo_loss: 341.15, cls_loss: 1.30, overall_loss: 341.15\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 15/200: Loss 339.1794  recon_loss: 339.11, kl_loss: 0.07, vaub_loss: 339.18, elbo_loss: 339.18, cls_loss: 1.29, overall_loss: 339.18\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 20/200: Loss 309.2220  recon_loss: 309.18, kl_loss: 0.04, vaub_loss: 309.22, elbo_loss: 309.22, cls_loss: 1.25, overall_loss: 309.22\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 25/200: Loss 331.4451  recon_loss: 331.39, kl_loss: 0.06, vaub_loss: 331.45, elbo_loss: 331.45, cls_loss: 1.30, overall_loss: 331.45\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 30/200: Loss 323.5920  recon_loss: 323.56, kl_loss: 0.03, vaub_loss: 323.59, elbo_loss: 323.59, cls_loss: 1.25, overall_loss: 323.59\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 35/200: Loss 321.1626  recon_loss: 321.14, kl_loss: 0.02, vaub_loss: 321.16, elbo_loss: 321.16, cls_loss: 1.28, overall_loss: 321.16\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 40/200: Loss 316.4285  recon_loss: 316.41, kl_loss: 0.02, vaub_loss: 316.43, elbo_loss: 316.43, cls_loss: 1.26, overall_loss: 316.43\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 45/200: Loss 336.1104  recon_loss: 336.09, kl_loss: 0.02, vaub_loss: 336.11, elbo_loss: 336.11, cls_loss: 1.27, overall_loss: 336.11\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 50/200: Loss 312.8416  recon_loss: 312.83, kl_loss: 0.01, vaub_loss: 312.84, elbo_loss: 312.84, cls_loss: 1.26, overall_loss: 312.84\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 55/200: Loss 317.8663  recon_loss: 317.86, kl_loss: 0.01, vaub_loss: 317.87, elbo_loss: 317.87, cls_loss: 1.23, overall_loss: 317.87\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 60/200: Loss 326.4584  recon_loss: 326.45, kl_loss: 0.01, vaub_loss: 326.46, elbo_loss: 326.46, cls_loss: 1.26, overall_loss: 326.46\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 65/200: Loss 329.2745  recon_loss: 321.98, kl_loss: 7.29, vaub_loss: 329.27, elbo_loss: 329.27, cls_loss: 1.28, overall_loss: 329.27\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 70/200: Loss 311.3469  recon_loss: 311.34, kl_loss: 0.01, vaub_loss: 311.35, elbo_loss: 311.35, cls_loss: 1.23, overall_loss: 311.35\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 75/200: Loss 301.2683  recon_loss: 301.26, kl_loss: 0.01, vaub_loss: 301.27, elbo_loss: 301.27, cls_loss: 1.22, overall_loss: 301.27\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 80/200: Loss 320.9874  recon_loss: 320.97, kl_loss: 0.02, vaub_loss: 320.99, elbo_loss: 320.99, cls_loss: 1.26, overall_loss: 320.99\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 85/200: Loss 316.1641  recon_loss: 315.68, kl_loss: 0.48, vaub_loss: 316.16, elbo_loss: 316.16, cls_loss: 1.26, overall_loss: 316.16\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 90/200: Loss 325.5554  recon_loss: 324.96, kl_loss: 0.60, vaub_loss: 325.56, elbo_loss: 325.56, cls_loss: 1.26, overall_loss: 325.56\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 95/200: Loss 327.5023  recon_loss: 323.08, kl_loss: 4.42, vaub_loss: 327.50, elbo_loss: 327.50, cls_loss: 1.23, overall_loss: 327.50\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 100/200: Loss 320.2961  recon_loss: 319.72, kl_loss: 0.58, vaub_loss: 320.30, elbo_loss: 320.30, cls_loss: 1.22, overall_loss: 320.30\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 105/200: Loss 304.7346  recon_loss: 304.16, kl_loss: 0.58, vaub_loss: 304.73, elbo_loss: 304.73, cls_loss: 1.22, overall_loss: 304.73\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 110/200: Loss 318.2856  recon_loss: 317.62, kl_loss: 0.67, vaub_loss: 318.29, elbo_loss: 318.29, cls_loss: 1.22, overall_loss: 318.29\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 115/200: Loss 323.6275  recon_loss: 323.11, kl_loss: 0.52, vaub_loss: 323.63, elbo_loss: 323.63, cls_loss: 1.23, overall_loss: 323.63\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 120/200: Loss 335.9930  recon_loss: 335.35, kl_loss: 0.65, vaub_loss: 335.99, elbo_loss: 335.99, cls_loss: 1.22, overall_loss: 335.99\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 125/200: Loss 345.3752  recon_loss: 344.46, kl_loss: 0.92, vaub_loss: 345.38, elbo_loss: 345.38, cls_loss: 1.24, overall_loss: 345.38\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 130/200: Loss 300.1730  recon_loss: 299.50, kl_loss: 0.67, vaub_loss: 300.17, elbo_loss: 300.17, cls_loss: 1.23, overall_loss: 300.17\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 135/200: Loss 347.6422  recon_loss: 346.84, kl_loss: 0.80, vaub_loss: 347.64, elbo_loss: 347.64, cls_loss: 1.19, overall_loss: 347.64\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 140/200: Loss 312.6957  recon_loss: 311.78, kl_loss: 0.92, vaub_loss: 312.70, elbo_loss: 312.70, cls_loss: 1.20, overall_loss: 312.70\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 145/200: Loss 316.4701  recon_loss: 315.35, kl_loss: 1.12, vaub_loss: 316.47, elbo_loss: 316.47, cls_loss: 1.27, overall_loss: 316.47\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 150/200: Loss 314.0643  recon_loss: 313.03, kl_loss: 1.03, vaub_loss: 314.06, elbo_loss: 314.06, cls_loss: 1.22, overall_loss: 314.06\n",
      "Overall predicted error = 0.25, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 155/200: Loss 332.6251  recon_loss: 331.47, kl_loss: 1.16, vaub_loss: 332.63, elbo_loss: 332.63, cls_loss: 1.22, overall_loss: 332.63\n",
      "Overall predicted error = 0.24, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 160/200: Loss 308.6514  recon_loss: 307.31, kl_loss: 1.34, vaub_loss: 308.65, elbo_loss: 308.65, cls_loss: 1.20, overall_loss: 308.65\n",
      "Overall predicted error = 0.24, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 165/200: Loss 307.2209  recon_loss: 305.79, kl_loss: 1.43, vaub_loss: 307.22, elbo_loss: 307.22, cls_loss: 1.21, overall_loss: 307.22\n",
      "Overall predicted error = 0.24, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 170/200: Loss 325.6237  recon_loss: 323.89, kl_loss: 1.73, vaub_loss: 325.62, elbo_loss: 325.62, cls_loss: 1.24, overall_loss: 325.62\n",
      "Overall predicted error = 0.24, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 175/200: Loss 292.6860  recon_loss: 291.42, kl_loss: 1.27, vaub_loss: 292.69, elbo_loss: 292.69, cls_loss: 1.24, overall_loss: 292.69\n",
      "Overall predicted error = 0.24, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 180/200: Loss 321.6685  recon_loss: 320.22, kl_loss: 1.45, vaub_loss: 321.67, elbo_loss: 321.67, cls_loss: 1.25, overall_loss: 321.67\n",
      "Overall predicted error = 0.24, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 185/200: Loss 327.3093  recon_loss: 325.69, kl_loss: 1.61, vaub_loss: 327.31, elbo_loss: 327.31, cls_loss: 1.21, overall_loss: 327.31\n",
      "Overall predicted error = 0.24, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 190/200: Loss 308.8649  recon_loss: 307.44, kl_loss: 1.43, vaub_loss: 308.86, elbo_loss: 308.86, cls_loss: 1.21, overall_loss: 308.86\n",
      "Overall predicted error = 0.24, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Epoch 195/200: Loss 326.1454  recon_loss: 324.65, kl_loss: 1.50, vaub_loss: 326.15, elbo_loss: 326.15, cls_loss: 1.23, overall_loss: 326.15\n",
      "Overall predicted error = 0.24, Err|A=0 = 0.31, Err|A=1 = 0.11\n",
      "Overall Error: 0.2442\n",
      "Joint Error: |Err|A=0 + Err|A=1| = 0.42085272807659424\n",
      "Error Gap: |Err|A=0 - Err|A=1| = 0.1941073586485461\n",
      "DP Gap: |Pred=1|A=0 - Pred=1|A=1| = 2.2667e-03\n",
      "Equalized Odds Y = 0: |Pred = 1|A = 0, Y = 0 - Pred = 1|A = 1, Y = 0| = 0.0\n",
      "Equalized Odds Y = 1: |Pred = 1|A = 0, Y = 1 - Pred = 1|A = 1, Y = 1| = 0.007317849188673242\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "#####     Training hyperparameters     #####\n",
    "############################################\n",
    "data_dir = \"/local/scratch/a/gong123/AUB-CAUB/ICLR2020-CFair/data\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 1024\n",
    "n_epochs = 200\n",
    "lr = 1e-3\n",
    "input_features = 114\n",
    "hidden_features = 84\n",
    "latent_features = 60\n",
    "scale_recon = 1\n",
    "lambda_cls = 1e-15\n",
    "\n",
    "# kwargs_pz = {\n",
    "#     \"n_components\": 10,\n",
    "#     \"input_dim\": latent_features,\n",
    "#     }\n",
    "\n",
    "############################################\n",
    "#####         Training models          #####\n",
    "############################################\n",
    "pz = FixedGaussian(input_dim=latent_features, device=device)\n",
    "cls = Classifier(input_dim=latent_features)\n",
    "VAUB = LinearVAUB(\n",
    "    pz = pz,\n",
    "    cls = cls,\n",
    "    input_features=input_features,\n",
    "    hidden_features=hidden_features,\n",
    "    latent_features=latent_features,\n",
    "    pair_kl=True,\n",
    ")\n",
    "\n",
    "############################################\n",
    "#####          Training data           #####\n",
    "############################################\n",
    "train_dataset, test_dataset = get_dataset_adult(data_dir)\n",
    "train_loader, test_loader = get_loaders_adult(train_dataset, test_dataset, batch_size)\n",
    "\n",
    "############################################\n",
    "#####        Training script           #####\n",
    "############################################\n",
    "# for name, p in VAUB.named_parameters():\n",
    "#     if p.requires_grad == True:\n",
    "#         print(name)\n",
    "Loss_dict = train_VAUB_fair(VAUB, train_loader, test_dataset, n_epochs, lr, scale_recon, lambda_cls, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-15T14:10:18.965920Z",
     "end_time": "2023-05-15T14:21:16.089173Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([328, 60])\n",
      "torch.Size([138, 60])\n",
      "Sliced Wasserstein Distance: 1.8631256818771362\n"
     ]
    }
   ],
   "source": [
    "def sliced_wasserstein_distance(x, y, num_projections=10000, p=1):\n",
    "    # Generate random projections\n",
    "    projections = torch.randn(x.shape[1], num_projections).to(x.device)\n",
    "    # projections = projections / torch.std(projections, dim=0, keepdim=True)\n",
    "\n",
    "    # Compute sliced distances for x and y\n",
    "    x_projected = torch.matmul(x, projections)\n",
    "    y_projected = torch.matmul(y, projections)\n",
    "\n",
    "    # Sort the projected samples\n",
    "    x_sorted, _ = torch.sort(x_projected, dim=0)\n",
    "    y_sorted, _ = torch.sort(y_projected, dim=0)\n",
    "\n",
    "    # Compute the sliced Wasserstein distance\n",
    "    sliced_dist = torch.mean(torch.abs(x_sorted - y_sorted) ** p) ** (1 / p)\n",
    "\n",
    "    return sliced_dist\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(VAUB.z_arr[0].shape)\n",
    "print(VAUB.z_arr[1].shape)\n",
    "\n",
    "indices = torch.randperm(VAUB.z_arr[0].shape[0])[:VAUB.z_arr[1].shape[0]]\n",
    "distance = sliced_wasserstein_distance(VAUB.z_arr[0][indices], VAUB.z_arr[1])\n",
    "print(\"Sliced Wasserstein Distance:\", distance.item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-15T14:24:26.531079Z",
     "end_time": "2023-05-15T14:24:26.556967Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced Wasserstein Distance: 9.713080406188965\n"
     ]
    }
   ],
   "source": [
    "def sliced_wasserstein_distance_whitening(model, test_loader, num_projections=1000, p=1):\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    dist_list = []\n",
    "\n",
    "    for _, (x_test, _, attrs) in enumerate(test_loader):\n",
    "\n",
    "        # x_test = torch.from_numpy(x_test).float().to(device)\n",
    "        x_test = x_test.float().to(device)\n",
    "        z_test = model.E_arr[0](x_test).cpu().detach().numpy()\n",
    "        # print(x_test.shape, z_test.shape)\n",
    "\n",
    "        # centering, unit variance\n",
    "        scaler = StandardScaler() # zero mean (feature-wise), unit-var\n",
    "        scaler.fit(z_test)\n",
    "\n",
    "        z_test_zm = scaler.transform(z_test)\n",
    "\n",
    "        # whitening\n",
    "        pca = PCA(whiten = True) # X_enrollment already unit variance\n",
    "        pca.fit(z_test_zm)\n",
    "        z_test_white = pca.transform(z_test_zm)\n",
    "\n",
    "        # Generate random projections\n",
    "        projections = torch.randn(z_test_white.shape[1], num_projections)\n",
    "        # projections = projections / torch.std(projections, dim=0, keepdim=True)\n",
    "\n",
    "        # Compute sliced distances for x and y\n",
    "\n",
    "        x, y = z_test_white[attrs==0], z_test_white[attrs==1]\n",
    "        indices = torch.randperm(x.shape[0])[:y.shape[0]]\n",
    "        x_truncated, y = torch.from_numpy(x[indices]), torch.from_numpy(y)\n",
    "\n",
    "        x_projected = torch.matmul(x_truncated, projections)\n",
    "        y_projected = torch.matmul(y, projections)\n",
    "\n",
    "        # Sort the projected samples\n",
    "        x_sorted, _ = torch.sort(x_projected, dim=0)\n",
    "        y_sorted, _ = torch.sort(y_projected, dim=0)\n",
    "\n",
    "        # Compute the sliced Wasserstein distance\n",
    "        dist_list.append(torch.mean(torch.abs(x_sorted - y_sorted) ** p) ** (1 / p))\n",
    "\n",
    "    return torch.tensor(dist_list).mean()\n",
    "\n",
    "distance = sliced_wasserstein_distance_whitening(VAUB, test_loader)\n",
    "print(\"Sliced Wasserstein Distance:\", distance.item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-15T14:24:29.391682Z",
     "end_time": "2023-05-15T14:24:31.583154Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'C': 10.0, 'gamma': 0.1}\n",
      "Best score:  0.9958498645294529\n",
      "Test accuracy:  0.997675962815405\n"
     ]
    }
   ],
   "source": [
    "def svm_acc(model, test_dataset):\n",
    "\n",
    "    from sklearn import svm, datasets\n",
    "    from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    test_loader_whole = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "    _, (X, y, a) = next(enumerate(test_loader_whole))\n",
    "\n",
    "    X_train, X_test, a_train, a_test = train_test_split(X, a, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train, X_test = X_train.float().to(device), X_test.float().to(device)\n",
    "    Z_train = model.E_arr[0](X_train).cpu().detach().numpy()\n",
    "    Z_test = model.E_arr[0](X_test).cpu().detach().numpy()\n",
    "    # print(x_test.shape, z_test.shape)\n",
    "\n",
    "    # centering, unit variance\n",
    "    scaler = StandardScaler() # zero mean (feature-wise), unit-var\n",
    "    scaler.fit(Z_train)\n",
    "    Z_train_zm = scaler.transform(Z_train)\n",
    "    Z_test_zm = scaler.transform(Z_test)\n",
    "\n",
    "    svm_classifier = svm.SVC()\n",
    "    parameters = {'C': np.logspace(-1, 1, 3), 'gamma': np.logspace(-1, 1, 3)}\n",
    "    grid_search = GridSearchCV(svm_classifier, parameters)\n",
    "\n",
    "    grid_search.fit(Z_train_zm, a_train)\n",
    "\n",
    "    # Print the best parameters and the corresponding score\n",
    "    print(\"Best parameters: \", grid_search.best_params_)\n",
    "    print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    best_model = grid_search.best_estimator_\n",
    "    a_pred = best_model.predict(Z_test_zm)\n",
    "    accuracy = accuracy_score(a_test, a_pred)\n",
    "    print(\"Test accuracy: \", accuracy)\n",
    "\n",
    "svm_acc(VAUB, test_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-15T15:14:39.934478Z",
     "end_time": "2023-05-15T15:18:55.025439Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
